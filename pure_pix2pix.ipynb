{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# My Pure Pix2Pix Implementation\n",
    "\n",
    "****\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General Presets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "\"\"\" Import Packages \"\"\"\n",
    "\n",
    "# Numerical Operations\n",
    "import random\n",
    "import numpy as np\n",
    "# Reading/Writing/Cleaning Data\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import gc\n",
    "# For Progress Bar\n",
    "from tqdm.auto import tqdm\n",
    "# For Display\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb as wb\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset, random_split\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "# my_utilities\n",
    "import my_utilities\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-12T11:04:49.288973Z",
     "end_time": "2023-10-12T11:04:49.835096Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:8cmu9zed) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f576c5e6a624af9a4a6d4bcff4a917e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">run0</strong> at: <a href='https://wandb.ai/cullencatniss/pure_pix2pix/runs/8cmu9zed' target=\"_blank\">https://wandb.ai/cullencatniss/pure_pix2pix/runs/8cmu9zed</a><br/> View job at <a href='https://wandb.ai/cullencatniss/pure_pix2pix/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNTA5MTQyNQ==/version_details/v2' target=\"_blank\">https://wandb.ai/cullencatniss/pure_pix2pix/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNTA5MTQyNQ==/version_details/v2</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20231012_102329-8cmu9zed\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:8cmu9zed). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f14d8c0adea44cdea9c4ec556e9e5e32"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>D:\\CAMPUS\\AI\\Essays\\PIX2PIX&CycleGAN\\pytorch-CycleGAN-and-pix2pix_fork\\wandb\\run-20231012_102428-wk8djyzo</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/cullencatniss/pure_pix2pix/runs/wk8djyzo' target=\"_blank\">run0</a></strong> to <a href='https://wandb.ai/cullencatniss/pure_pix2pix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/cullencatniss/pure_pix2pix' target=\"_blank\">https://wandb.ai/cullencatniss/pure_pix2pix</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/cullencatniss/pure_pix2pix/runs/wk8djyzo' target=\"_blank\">https://wandb.ai/cullencatniss/pure_pix2pix/runs/wk8djyzo</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cullencatniss/pure_pix2pix/runs/wk8djyzo?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x1a6bfc03f50>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" For Display \"\"\"\n",
    "\n",
    "plt.style.use(r'C:\\Users\\chael\\.matplotlib\\violet.mplstyle')\n",
    "wb.init(project=\"pure_pix2pix\", name=\"run0\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-12T10:24:27.969679Z",
     "end_time": "2023-10-12T10:26:52.120166Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\"\"\" For Auto-reload Modules\"\"\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-12T10:25:18.834478Z",
     "end_time": "2023-10-12T10:26:52.121752Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\"\"\" Assure Reproducibility \"\"\"\n",
    "\n",
    "seed = 29\n",
    "my_utilities.assure_reproduce(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-12T10:25:19.254845Z",
     "end_time": "2023-10-12T10:26:52.121752Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configurations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "\"\"\" Path & Device\"\"\"\n",
    "\n",
    "dataset_name = 'facades/'\n",
    "task_name = 'facades_pix2pix/'\n",
    "data_dir, checkpoints_dir, results_dir = 'datasets/' + dataset_name, 'checkpoints/' + task_name, 'results/' + task_name\n",
    "device = my_utilities.register_device()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-12T10:35:00.974728Z",
     "end_time": "2023-10-12T10:35:01.431191Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "\"\"\" Loading & saving Rules \"\"\"\n",
    "\n",
    "# 'data_load_threads' =\n",
    "# 'epoch_load' =\n",
    "model_load = 'by iters.'  # | by_epochs\n",
    "results_save = 5000  # frequency of saving latest result (by iters.)\n",
    "checkpoints_save = 5  # frequency of saving latest checkpoints (by epochs)\n",
    "model_save = 'by iters.'  # | by_epochs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-12T10:35:01.433337Z",
     "end_time": "2023-10-12T10:35:01.815390Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "\"\"\" Preprocessing Rules \"\"\"\n",
    "\n",
    "preproc_train = {\n",
    "\t'trans': 'scale & crop',  # | crop | scale width | scale width & crop | none\n",
    "\t'scale_size': 286,  # !!! in TEST TIME set to crop_size !!!\n",
    "\t'crop_size': 256,\n",
    "\t'flip': False  # whether to flip images in augmentation\n",
    "}\n",
    "preproc_test = {\n",
    "\t'trans': preproc_train['trans'],\n",
    "\t'scale_size': preproc_train['crop_size'],\n",
    "\t'crop_size': preproc_train['crop_size'],\n",
    "\t'flip': preproc_train['flip']\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-12T16:23:36.443480Z",
     "end_time": "2023-10-12T16:23:37.212128Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "\"\"\" Model \"\"\"\n",
    "\n",
    "model_name = 'pix2pix'\n",
    "translate_dirct = 'facades->photo'  # | 'photo->facades'\n",
    "\n",
    "hyper_params = {\n",
    "\t# ======= network architectures =======\n",
    "\t'G_arch': 'U-net256',  # | U-net128\n",
    "\t'D_arch': 'PatchGAN',  # (for experiments in the paper) | PixelGAN | StdCNN\n",
    "\t# 'G_layers':\n",
    "\t'D_layers': 3,\n",
    "\t'in_chan': 3,\n",
    "\t'out_chan': 3,\n",
    "\t'G_fil_num': 64,  # num. of filters in G's first conv. layer\n",
    "\t'D_fil_num': 64,  # num. of filters in D's first conv. layer\n",
    "\t'batch_size': 32,\n",
    "\t'norm_type': 'instance',  # (for experiments in the paper) | batch | none\n",
    "\t'G_dropout': True,\n",
    "\t'D_dropout': False,  # no dropout for G in the paper\n",
    "\t# ============== training ==============\n",
    "\t# 'init_type':\n",
    "\t'init_scaler': 0.02,  # scaling factor for weight initialization\n",
    "\t'shuffle_batch': True,\n",
    "\t'continue_train': True,  # whether to load latest model\n",
    "\t'loss_mode': 'vanilla GAN',  # | lsgan | wgangp\n",
    "\t'L1_lambda': 100,  # L1's scaler (add L1 to GAN-loss according to the paper)\n",
    "\t'beta1': 0.5,  # momentum for Adam (0.9?)\n",
    "\t# 'beta2': (?)\n",
    "\t'lr': 0.0002,  # initial learning rate for Adam (0.001?)\n",
    "\t'lr_dec_mode': 'linear',  # | step | plateau | cosine\n",
    "\t'lr_dec_iters': 50,  # multiply by a gamma every lr_decay_iters iterations\n",
    "\t'init_lr_epochs': 100,\n",
    "\t'decay_lr_epochs': 100,  # linear decay to 0\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-12T16:23:37.217600Z",
     "end_time": "2023-10-12T16:23:37.623933Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============CONFIGURATIONS============\n",
      "Device = cuda\n",
      "Model is loaded by iters.\n",
      "Model is saved by iters.\n",
      "Model = pix2pix\n",
      "Translation Dirct. = facades->photo\n",
      "--------------------------------------\n",
      "Transforms Opt. = scale & crop\n",
      "Img. Scale Size = 256\n",
      "Img. Crop Size = 256\n",
      "Flipped = False\n",
      "--------------------------------------\n",
      "G's Arch. = U-net256\n",
      "D's Arch. = PatchGAN\n",
      "num. of D's layers = 3\n",
      "num. of Input's Channels = 3\n",
      "num. of Output's Channels = 3\n",
      "--------------------------------------\n",
      "Batch Size = 32\n",
      "Normalization = instance\n",
      "G's Dropout = False\n",
      "D's Dropout = True\n",
      "--------------------------------------\n",
      "Loss Mode = vanilla GAN\n",
      "L1's Lambda = 100\n",
      "Beta1 = 0.5\n",
      "Initial Learning-rate = 0.0002\n",
      "Decay Mode = linear\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "wb.config.update({'translate_dirct': translate_dirct, **preproc_train, **hyper_params})\n",
    "my_utilities.print_config(device, model_load, model_save, model_name, translate_dirct,\n",
    "                          {**preproc_train, **hyper_params})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-12T16:23:37.625944Z",
     "end_time": "2023-10-12T16:23:38.077033Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "\"\"\" Create dataset (`__get_item__` includes transforms) \"\"\"\n",
    "\n",
    "raw_train_set = my_utilities.read_img(data_dir, 'train')\n",
    "raw_test_set = my_utilities.read_img(data_dir, 'test')\n",
    "train_set = my_utilities.Imageset(preproc_train, raw_train_set)\n",
    "# train_set.check_preproc(0)\n",
    "test_set = my_utilities.Imageset(preproc_test, raw_test_set)\n",
    "# test_set.check_preproc(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-12T16:27:17.118529Z",
     "end_time": "2023-10-12T16:27:18.058336Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "\"\"\" Load data to batches \"\"\"\n",
    "\n",
    "train_batches = DataLoader(train_set, hyper_params['batch_size'], hyper_params['shuffle_batch'], pin_memory=True)\n",
    "test_batches = DataLoader(test_set, hyper_params['batch_size'], hyper_params['shuffle_batch'], pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-12T16:56:04.425659Z",
     "end_time": "2023-10-12T16:56:04.884770Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\" Define Generator archs. \"\"\"\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mUnet\u001B[39;00m(nn\u001B[38;5;241m.\u001B[39mModule):\n\u001B[0;32m      5\u001B[0m \t\u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m, in_chan\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, out_chan\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, fst_filters\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m'\u001B[39m, dropout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m      6\u001B[0m \u001B[38;5;250m\t\t\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124;03m\t\t:param img_size: width (or height) of input img.\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;124;03m\t\t:param in_chan: number of input channels\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124;03m\t\t:param dropout: whether to use dropout\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;124;03m\t\t\"\"\"\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" Define Generator arch. \"\"\"\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "\tdef __init__(self, img_size=256, in_chan=3, out_chan=3, fst_filters=64, norm='batch', dropout=True):\n",
    "\t\t\"\"\"\n",
    "\t\t:param img_size: width (or height) of input img.\n",
    "\t\t:param in_chan: number of input channels\n",
    "\t\t:param out_chan: number of output channels\n",
    "\t\t:param fst_filters: number of filters in the first conv.\n",
    "\t\t:param norm: type of conv.'s normalization\n",
    "\t\t:param dropout: whether to use dropout\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.encoder = []  # a list of layers in encoder\n",
    "\t\tself.decoder = []  # a list of layers in decoder\n",
    "\t\tself.N = img_size\n",
    "\t\tself.in_chan = in_chan\n",
    "\t\tself.out_chan = out_chan\n",
    "\t\tself.fst_filter = fst_filters\n",
    "\t\tnorm_opt = {'batch': nn.BatchNorm2d, 'instance': nn.InstanceNorm2d}\n",
    "\t\tself.norm = norm_opt[norm]\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.drop_rate = 0.5\n",
    "\t\tself.leak_rate = 0.2\n",
    "\t\tself.F, self.P, self.S = 4, 2, 1  # filter_size, padding_width, step_size\n",
    "\t\tself.layers_num = 0\n",
    "\n",
    "\tdef check_layers(self):\n",
    "\t\twhile n != 1:\n",
    "\t\t\tself.N = (self.N - self.F + 2 * self.P) / self.S + 1  # simulate the change of img. size\n",
    "\t\t\tassert (self.N - int(self.N) == 0)\n",
    "\t\t\tself.layers_num = self.layers_num + 1\n",
    "\n",
    "\tdef pile_encoder(self):\n",
    "\t\tn = len(self.encoder)\n",
    "\t\tif n < self.layers_num:\n",
    "\t\t\t# set in|out channels\n",
    "\t\t\tif n == 0:\n",
    "\t\t\t\tlayer_in_chan, layer_out_chan = self.in_chan, self.fst_filter\n",
    "\t\t\telif n < 4:\n",
    "\t\t\t\tlayer_in_chan, layer_out_chan = self.fst_filter * (2 ** (n - 1)), self.fst_filter * (2 ** n)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlayer_in_chan, layer_out_chan = self.fst_filter * 8, self.fst_filter * 8\n",
    "\t\t\t# construct layer\n",
    "\t\t\tlayer = []\n",
    "\t\t\tlayer += [nn.LeakyReLU(self.leak_rate, inplace=True)] if n != 0 else []\n",
    "\t\t\tlayer += [nn.Conv2d(layer_in_chan, layer_out_chan, self.F, self.S, self.P)]\n",
    "\t\t\tlayer += [self.norm(layer_out_chan)] if n != 0 else []\n",
    "\t\t\t# add layer\n",
    "\t\t\tself.encoder.append(nn.Sequential(*layer))\n",
    "\t\t\tprint(\"Encoder-layer NO.\" + str(n) + \" added\")  # layer_no is the index in encoder[]\n",
    "\t\t\t# add another\n",
    "\t\t\tself.pile_encoder()\n",
    "\n",
    "\t\treturn  # if len(self.encoder)==self.layers_num then the encoder is already intact\n",
    "\n",
    "\tdef pile_decoder(self):  # pile decoder reversely for convenience\n",
    "\t\tn = len(self.decoder)\n",
    "\t\tif n < self.layers_num:\n",
    "\t\t\t# set in|out channels\n",
    "\t\t\tif n == 0:\n",
    "\t\t\t\tlayer_in_chan, layer_out_chan = 2 * self.fst_filter, self.out_chan\n",
    "\t\t\telif n < 4:\n",
    "\t\t\t\tlayer_in_chan, layer_out_chan = 2 * (self.fst_filter * (2 ** n)), self.fst_filter * (2 ** (n - 1))\n",
    "\t\t\telse:\n",
    "\t\t\t\tlayer_in_chan, layer_out_chan = 2 * (self.fst_filter * 8), self.fst_filter * 8\n",
    "\t\t\t# construct layer\n",
    "\t\t\tlayer = []\n",
    "\t\t\tlayer += [nn.ReLU(inplace=True)] if n != 0 else []\n",
    "\t\t\tlayer += [nn.ConvTranspose2d(layer_in_chan, layer_out_chan, self.F, self.S, self.P)]\n",
    "\t\t\tlayer += [self.norm(layer_out_chan)] if n != 0 else []\n",
    "\t\t\tlayer += [nn.Dropout(0.5)] if n != 0 and self.dropout == True else []\n",
    "\t\t\t# add layer\n",
    "\t\t\tself.decoder.append(nn.Sequential(*layer))\n",
    "\t\t\tprint(\"Decoder-layer NO.\" + str(n) + \" added\")\n",
    "\t\t\t# add another\n",
    "\t\t\tself.pile_decoder()\n",
    "\n",
    "\t\treturn  # if len(self.decoder)==self.layers_num then the reversed decoder is already intact\n",
    "\n",
    "\tdef en_forward(self, x, i):\n",
    "\t\tself.encoder[i] = self.encoder[i](x) if i == 0 else self.encoder[i](en_forward(x, i - 1))\n",
    "\t\t# (reuse self.encoder for inner results of encoder)\n",
    "\t\tprint(\"Encoder-layer NO.\" + str(n) + \" done\")\n",
    "\t\treturn self.encoder[i]\n",
    "\n",
    "\tdef de_forward(self, x, i):\n",
    "\t\tres = self.decoder[i](x) if i == self.layers_num - 1 else self.decoder[i](de_forward(x, i + 1))\n",
    "\t\tprint(\"Decoder-layer NO.\" + str(n) + \" done\")\n",
    "\t\treturn torch.cat([res, self.encoder[i - 1]], 1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tself.check_layers()\n",
    "\t\tself.pile_encoder()\n",
    "\t\tself.pile_decoder()\n",
    "\t\tlatent = self.en_forward(x, self.layers_num - 1)\n",
    "\t\ty = self.de_forward(latent, 0)\n",
    "\t\treturn y\n",
    "\n",
    "\n",
    "G = Unet(\n",
    "\tpreproc_train['crop_size'],\n",
    "\thyper_params['in_chan'], hyper_params['out_chan'],\n",
    "\thyper_params['G_fil_num'], hyper_params['norm_type'], hyper_params['G_dropout']\n",
    ")\n",
    "G.forward()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
